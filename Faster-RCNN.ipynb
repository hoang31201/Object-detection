{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0-1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"2SmPxLdegBT1","executionInfo":{"status":"ok","timestamp":1621508334725,"user_tz":-600,"elapsed":1565,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}}},"source":["import os\n","import re"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KFtUi1hRrYtA","executionInfo":{"status":"ok","timestamp":1621508394060,"user_tz":-600,"elapsed":60892,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}},"outputId":"09423d06-8ca5-4ceb-bfbc-f5f0a1899af2"},"source":["# Install Tensorflow GPU 1.15\n","!pip install tensorflow_gpu==1.15"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting tensorflow_gpu==1.15\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/72/d06017379ad4760dc58781c765376ce4ba5dcf3c08d37032eeefbccf1c51/tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5MB)\n","\u001b[K     |████████████████████████████████| 411.5MB 41kB/s \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (0.2.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (1.15.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (1.1.2)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (1.32.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (0.8.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (3.3.0)\n","Collecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n","\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (1.19.5)\n","Collecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |████████████████████████████████| 512kB 45.8MB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (1.1.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (0.12.0)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (3.12.4)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (0.36.2)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15) (1.12.1)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 41.8MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow_gpu==1.15) (2.10.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow_gpu==1.15) (56.1.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (3.3.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (2.0.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (4.0.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15) (3.4.1)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=9d9746ce6db4d92d81775fd5f2e856d14d9aa99edd89472f6c7ed09a7385127d\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow 2.4.1 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorboard~=2.4, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.4.1 has requirement tensorflow-estimator<2.5.0,>=2.4.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: keras-applications, tensorflow-estimator, gast, tensorboard, tensorflow-gpu\n","  Found existing installation: tensorflow-estimator 2.4.0\n","    Uninstalling tensorflow-estimator-2.4.0:\n","      Successfully uninstalled tensorflow-estimator-2.4.0\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","  Found existing installation: tensorboard 2.4.1\n","    Uninstalling tensorboard-2.4.1:\n","      Successfully uninstalled tensorboard-2.4.1\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"er-bY2cvraLE","executionInfo":{"status":"ok","timestamp":1621508394061,"user_tz":-600,"elapsed":60892,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}}},"source":["# Parameter, model etc. Setup\n","# Repository link to clone/use\n","repo_url = 'https://github.com/Sern5011/BloodRBC'\n","\n","# Number of training steps.\n","num_steps = 10000  # 200000 Increase the steps as required.\n","\n","# Number of evaluation steps.\n","num_eval_steps = 50\n","\n","#Define the models to use (Already available in the )\n","MODELS_CONFIG = {\n","    'ssd_mobilenet_v2': {\n","        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n","        'pipeline_file': 'ssd_mobilenet_v2_coco.config',\n","        'batch_size': 12\n","    },\n","    'faster_rcnn_inception_v2': {\n","        'model_name': 'faster_rcnn_inception_v2_coco_2018_01_28',\n","        'pipeline_file': 'faster_rcnn_inception_v2_pets.config',\n","        'batch_size': 12\n","    },\n","    'rfcn_resnet101': {\n","        'model_name': 'rfcn_resnet101_coco_2018_01_28',\n","        'pipeline_file': 'rfcn_resnet101_pets.config',\n","        'batch_size': 12\n","    }\n","}\n","\n","# Pick the model you want to use\n","selected_model = 'faster_rcnn_inception_v2'\n","\n","# Name of the object detection model to use.\n","MODEL = MODELS_CONFIG[selected_model]['model_name']\n","\n","# Name of the pipline file in tensorflow object detection API.\n","pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n","\n","# Training batch size fits in Colab's GPU memory for selected model.\n","batch_size = MODELS_CONFIG[selected_model]['batch_size']"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YyxMmRw6PJfA","executionInfo":{"status":"ok","timestamp":1621508396325,"user_tz":-600,"elapsed":63152,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}},"outputId":"3e51d779-cc57-4ec7-c7df-31d8353f38f8"},"source":["%cd /content\n","\n","repo_dir_path = os.path.abspath(os.path.join('.', os.path.basename(repo_url)))\n","\n","!git clone {repo_url}\n","%cd {repo_dir_path}\n","!git pull"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content\n","Cloning into 'BloodRBC'...\n","remote: Enumerating objects: 721, done.\u001b[K\n","remote: Counting objects: 100% (721/721), done.\u001b[K\n","remote: Compressing objects: 100% (379/379), done.\u001b[K\n","remote: Total 721 (delta 344), reused 710 (delta 339), pack-reused 0\u001b[K\n","Receiving objects: 100% (721/721), 6.57 MiB | 35.81 MiB/s, done.\n","Resolving deltas: 100% (344/344), done.\n","/content/BloodRBC\n","Already up to date.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gH_z_isPSSXt","executionInfo":{"status":"ok","timestamp":1621508432953,"user_tz":-600,"elapsed":99776,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}},"outputId":"344ab549-e508-4769-c575-018da25c3879"},"source":["%cd /content\n","!git clone --quiet https://github.com/tensorflow/models.git\n","\n","!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n","\n","!pip install -q Cython contextlib2 pillow lxml matplotlib pycocotools tf_slim\n","\n","%cd /content/models/research\n","!protoc object_detection/protos/*.proto --python_out=.\n","\n","import os\n","os.environ['PYTHONPATH'] += ':/content/models/research/:/content/models/research/slim/'\n","\n","!python object_detection/builders/model_builder_test.py"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/content\n","Selecting previously unselected package python-bs4.\n","(Reading database ... 160706 files and directories currently installed.)\n","Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n","Unpacking python-bs4 (4.6.0-1) ...\n","Selecting previously unselected package python-pkg-resources.\n","Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n","Unpacking python-pkg-resources (39.0.1-2) ...\n","Selecting previously unselected package python-chardet.\n","Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n","Unpacking python-chardet (3.0.4-1) ...\n","Selecting previously unselected package python-six.\n","Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n","Unpacking python-six (1.11.0-2) ...\n","Selecting previously unselected package python-webencodings.\n","Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n","Unpacking python-webencodings (0.5-2) ...\n","Selecting previously unselected package python-html5lib.\n","Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n","Unpacking python-html5lib (0.999999999-1) ...\n","Selecting previously unselected package python-lxml:amd64.\n","Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.4_amd64.deb ...\n","Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.4) ...\n","Selecting previously unselected package python-olefile.\n","Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n","Unpacking python-olefile (0.45.1-1) ...\n","Selecting previously unselected package python-pil:amd64.\n","Preparing to unpack .../8-python-pil_5.1.0-1ubuntu0.5_amd64.deb ...\n","Unpacking python-pil:amd64 (5.1.0-1ubuntu0.5) ...\n","Setting up python-pkg-resources (39.0.1-2) ...\n","Setting up python-six (1.11.0-2) ...\n","Setting up python-bs4 (4.6.0-1) ...\n","Setting up python-lxml:amd64 (4.2.1-1ubuntu0.4) ...\n","Setting up python-olefile (0.45.1-1) ...\n","Setting up python-pil:amd64 (5.1.0-1ubuntu0.5) ...\n","Setting up python-webencodings (0.5-2) ...\n","Setting up python-chardet (3.0.4-1) ...\n","Setting up python-html5lib (0.999999999-1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","\u001b[K     |████████████████████████████████| 358kB 28.1MB/s \n","\u001b[?25h/content/models/research\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r0_e5Nd-EB4W","executionInfo":{"status":"ok","timestamp":1621508438211,"user_tz":-600,"elapsed":105031,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}},"outputId":"3e54e903-e812-4b2f-90e0-8c922296d6c8"},"source":["%cd {repo_dir_path}\n","\n","!python xml_to_csv.py -i data/train -o data/annotations/train_labels.csv -l data/annotations\n","\n","!python xml_to_csv.py -i data/test -o data/annotations/test_labels.csv\n","\n","!python generate_tfrecord.py --csv_input=data/annotations/train_labels.csv --output_path=data/annotations/train.record --img_path=data/train --label_map data/annotations/label_map.pbtxt\n","\n","!python generate_tfrecord.py --csv_input=data/annotations/test_labels.csv --output_path=data/annotations/test.record --img_path=data/test --label_map data/annotations/label_map.pbtxt"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/BloodRBC\n","Successfully converted xml to csv.\n","Generate `data/annotations/label_map.pbtxt`\n","Successfully converted xml to csv.\n","WARNING:tensorflow:From generate_tfrecord.py:134: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n","\n","WARNING:tensorflow:From generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n","\n","W0520 11:00:35.538828 140437940885376 module_wrapper.py:139] From generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n","\n","WARNING:tensorflow:From generate_tfrecord.py:53: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","W0520 11:00:35.585211 140437940885376 module_wrapper.py:139] From generate_tfrecord.py:53: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","Successfully created the TFRecords: /content/BloodRBC/data/annotations/train.record\n","WARNING:tensorflow:From generate_tfrecord.py:134: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n","\n","WARNING:tensorflow:From generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n","\n","W0520 11:00:38.006618 139945655261056 module_wrapper.py:139] From generate_tfrecord.py:107: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n","\n","WARNING:tensorflow:From generate_tfrecord.py:53: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","W0520 11:00:38.026185 139945655261056 module_wrapper.py:139] From generate_tfrecord.py:53: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n","\n","Successfully created the TFRecords: /content/BloodRBC/data/annotations/test.record\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NGWmkIyWJwyw","executionInfo":{"status":"ok","timestamp":1621508438212,"user_tz":-600,"elapsed":105030,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}}},"source":["# Define the Train, test records and the label map\n","test_record_fname = '/content/BloodRBC/data/annotations/test.record'\n","train_record_fname = '/content/BloodRBC/data/annotations/train.record'\n","label_map_pbtxt_fname = '/content/BloodRBC/data/annotations/label_map.pbtxt'"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4KMba_Q-IUp1","executionInfo":{"status":"ok","timestamp":1621508440786,"user_tz":-600,"elapsed":107599,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}},"outputId":"4df25179-eceb-4dc8-c1f7-91aa0d9a1647"},"source":["%cd /content/models/research\n","\n","import os\n","import shutil\n","import glob\n","import urllib.request\n","import tarfile\n","MODEL_FILE = MODEL + '.tar.gz'\n","DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n","DEST_DIR = '/content/models/research/pretrained_model'\n","\n","if not (os.path.exists(MODEL_FILE)):\n","    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n","\n","tar = tarfile.open(MODEL_FILE)\n","tar.extractall()\n","tar.close()\n","\n","os.remove(MODEL_FILE)\n","if (os.path.exists(DEST_DIR)):\n","    shutil.rmtree(DEST_DIR)\n","os.rename(MODEL, DEST_DIR)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/content/models/research\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q96F2UvYIbm3","executionInfo":{"status":"ok","timestamp":1621508441799,"user_tz":-600,"elapsed":108610,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}},"outputId":"d76b50b3-45a8-4694-82a1-2feab9754d86"},"source":["!echo {DEST_DIR}\n","!ls -alh {DEST_DIR}"],"execution_count":9,"outputs":[{"output_type":"stream","text":["/content/models/research/pretrained_model\n","total 111M\n","drwxr-xr-x  3 345018 5000 4.0K Feb  1  2018 .\n","drwxr-xr-x 23 root   root 4.0K May 20 11:00 ..\n","-rw-r--r--  1 345018 5000   77 Feb  1  2018 checkpoint\n","-rw-r--r--  1 345018 5000  55M Feb  1  2018 frozen_inference_graph.pb\n","-rw-r--r--  1 345018 5000  51M Feb  1  2018 model.ckpt.data-00000-of-00001\n","-rw-r--r--  1 345018 5000  16K Feb  1  2018 model.ckpt.index\n","-rw-r--r--  1 345018 5000 5.5M Feb  1  2018 model.ckpt.meta\n","-rw-r--r--  1 345018 5000 3.2K Feb  1  2018 pipeline.config\n","drwxr-xr-x  3 345018 5000 4.0K Feb  1  2018 saved_model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"mpCq7AO5IgtN","executionInfo":{"status":"ok","timestamp":1621508441800,"user_tz":-600,"elapsed":108606,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}},"outputId":"bdafb4aa-a0c8-4bb1-c503-3a96dc9a89b2"},"source":["fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n","fine_tune_checkpoint"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/models/research/pretrained_model/model.ckpt'"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"ism0Cw0yIndU","executionInfo":{"status":"ok","timestamp":1621508441800,"user_tz":-600,"elapsed":108605,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}}},"source":["import os\n","pipeline_fname = os.path.join('/content/models/research/object_detection/samples/configs/', pipeline_file)\n","\n","assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"jhGIdczSIp3p","executionInfo":{"status":"ok","timestamp":1621508441801,"user_tz":-600,"elapsed":108605,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}}},"source":["def get_num_classes(pbtxt_fname):\n","    from object_detection.utils import label_map_util\n","    label_map = label_map_util.load_labelmap(pbtxt_fname)\n","    categories = label_map_util.convert_label_map_to_categories(\n","        label_map, max_num_classes=90, use_display_name=True)\n","    category_index = label_map_util.create_category_index(categories)\n","    return len(category_index.keys())"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"wSGxHyDrItim","executionInfo":{"status":"ok","timestamp":1621508442331,"user_tz":-600,"elapsed":109134,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}}},"source":["num_classes = get_num_classes(label_map_pbtxt_fname)\n","with open(pipeline_fname) as f:\n","    s = f.read()\n","with open(pipeline_fname, 'w') as f:\n","    \n","    # fine_tune_checkpoint\n","    s = re.sub('fine_tune_checkpoint: \".*?\"',\n","               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n","    \n","    # tfrecord files train and test.\n","    s = re.sub(\n","        '(input_path: \".*?)(train.record)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n","    s = re.sub(\n","        '(input_path: \".*?)(val.record)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n","\n","    # label_map_path\n","    s = re.sub(\n","        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n","\n","    # Set training batch_size.\n","    s = re.sub('batch_size: [0-9]+',\n","               'batch_size: {}'.format(batch_size), s)\n","\n","    # Set training steps, num_steps\n","    s = re.sub('num_steps: [0-9]+',\n","               'num_steps: {}'.format(num_steps), s)\n","    \n","    # Set number of classes num_classes.\n","    s = re.sub('num_classes: [0-9]+',\n","               'num_classes: {}'.format(num_classes), s)\n","    f.write(s)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6r1CoqK4KpMj","executionInfo":{"status":"ok","timestamp":1621508442331,"user_tz":-600,"elapsed":109130,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}},"outputId":"2f454018-5f4b-49d7-d003-079e93473028"},"source":["print(pipeline_fname)\n","!cat {pipeline_fname}"],"execution_count":14,"outputs":[{"output_type":"stream","text":["/content/models/research/object_detection/samples/configs/faster_rcnn_inception_v2_pets.config\n","# Faster R-CNN with Inception v2, configured for Oxford-IIIT Pets Dataset.\n","# Users should configure the fine_tune_checkpoint field in the train config as\n","# well as the label_map_path and input_path fields in the train_input_reader and\n","# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n","# should be configured.\n","\n","model {\n","  faster_rcnn {\n","    num_classes: 1\n","    image_resizer {\n","      keep_aspect_ratio_resizer {\n","        min_dimension: 600\n","        max_dimension: 1024\n","      }\n","    }\n","    feature_extractor {\n","      type: 'faster_rcnn_inception_v2'\n","      first_stage_features_stride: 16\n","    }\n","    first_stage_anchor_generator {\n","      grid_anchor_generator {\n","        scales: [0.25, 0.5, 1.0, 2.0]\n","        aspect_ratios: [0.5, 1.0, 2.0]\n","        height_stride: 16\n","        width_stride: 16\n","      }\n","    }\n","    first_stage_box_predictor_conv_hyperparams {\n","      op: CONV\n","      regularizer {\n","        l2_regularizer {\n","          weight: 0.0\n","        }\n","      }\n","      initializer {\n","        truncated_normal_initializer {\n","          stddev: 0.01\n","        }\n","      }\n","    }\n","    first_stage_nms_score_threshold: 0.0\n","    first_stage_nms_iou_threshold: 0.7\n","    first_stage_max_proposals: 300\n","    first_stage_localization_loss_weight: 2.0\n","    first_stage_objectness_loss_weight: 1.0\n","    initial_crop_size: 14\n","    maxpool_kernel_size: 2\n","    maxpool_stride: 2\n","    second_stage_box_predictor {\n","      mask_rcnn_box_predictor {\n","        use_dropout: false\n","        dropout_keep_probability: 1.0\n","        fc_hyperparams {\n","          op: FC\n","          regularizer {\n","            l2_regularizer {\n","              weight: 0.0\n","            }\n","          }\n","          initializer {\n","            variance_scaling_initializer {\n","              factor: 1.0\n","              uniform: true\n","              mode: FAN_AVG\n","            }\n","          }\n","        }\n","      }\n","    }\n","    second_stage_post_processing {\n","      batch_non_max_suppression {\n","        score_threshold: 0.0\n","        iou_threshold: 0.6\n","        max_detections_per_class: 100\n","        max_total_detections: 300\n","      }\n","      score_converter: SOFTMAX\n","    }\n","    second_stage_localization_loss_weight: 2.0\n","    second_stage_classification_loss_weight: 1.0\n","  }\n","}\n","\n","train_config: {\n","  batch_size: 12\n","  optimizer {\n","    momentum_optimizer: {\n","      learning_rate: {\n","        manual_step_learning_rate {\n","          initial_learning_rate: 0.0002\n","          schedule {\n","            step: 900000\n","            learning_rate: .00002\n","          }\n","          schedule {\n","            step: 1200000\n","            learning_rate: .000002\n","          }\n","        }\n","      }\n","      momentum_optimizer_value: 0.9\n","    }\n","    use_moving_average: false\n","  }\n","  gradient_clipping_by_norm: 10.0\n","  fine_tune_checkpoint: \"/content/models/research/pretrained_model/model.ckpt\"\n","  from_detection_checkpoint: true\n","  load_all_detection_checkpoint_vars: true\n","  # Note: The below line limits the training process to 200K steps, which we\n","  # empirically found to be sufficient enough to train the pets dataset. This\n","  # effectively bypasses the learning rate schedule (the learning rate will\n","  # never decay). Remove the below line to train indefinitely.\n","  num_steps: 10000\n","  data_augmentation_options {\n","    random_horizontal_flip {\n","    }\n","  }\n","}\n","\n","\n","train_input_reader: {\n","  tf_record_input_reader {\n","    input_path: \"/content/BloodRBC/data/annotations/train.record\"\n","  }\n","  label_map_path: \"/content/BloodRBC/data/annotations/label_map.pbtxt\"\n","}\n","\n","eval_config: {\n","  metrics_set: \"coco_detection_metrics\"\n","  num_examples: 1101\n","}\n","\n","eval_input_reader: {\n","  tf_record_input_reader {\n","    input_path: \"/content/BloodRBC/data/annotations/test.record\"\n","  }\n","  label_map_path: \"/content/BloodRBC/data/annotations/label_map.pbtxt\"\n","  shuffle: false\n","  num_readers: 1\n","}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PvDnQGXJKqzb","executionInfo":{"status":"ok","timestamp":1621508442689,"user_tz":-600,"elapsed":109487,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}}},"source":["model_dir = 'training/'\n","# Optionally remove content in output model directory to fresh start.\n","!rm -rf {model_dir}\n","os.makedirs(model_dir, exist_ok=True)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kZmDIi3zK1-x","executionInfo":{"status":"ok","timestamp":1621508443228,"user_tz":-600,"elapsed":110022,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}},"outputId":"e0bec121-517b-43d0-8b89-e8f64f104168"},"source":["!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip -o ngrok-stable-linux-amd64.zip"],"execution_count":16,"outputs":[{"output_type":"stream","text":["--2021-05-20 11:00:42--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 3.224.116.172, 54.80.88.238, 54.225.197.119, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|3.224.116.172|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13832437 (13M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip’\n","\n","\r          ngrok-sta   0%[                    ]       0  --.-KB/s               \r         ngrok-stab  90%[=================>  ]  11.92M  59.3MB/s               \rngrok-stable-linux- 100%[===================>]  13.19M  62.3MB/s    in 0.2s    \n","\n","2021-05-20 11:00:43 (62.3 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xQNesOBfLCoZ","executionInfo":{"status":"ok","timestamp":1621508443228,"user_tz":-600,"elapsed":110021,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}}},"source":["LOG_DIR = model_dir\n","get_ipython().system_raw(\n","    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n","    .format(LOG_DIR)\n",")"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"hdrMUYRgLDwj","executionInfo":{"status":"ok","timestamp":1621508443229,"user_tz":-600,"elapsed":110021,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}}},"source":["get_ipython().system_raw('./ngrok http 6006 &')"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r7RGX9j-LGO2","executionInfo":{"status":"ok","timestamp":1621508443788,"user_tz":-600,"elapsed":110577,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}},"outputId":"4e4a9479-3270-4221-fc3d-2924f528fd5a"},"source":["! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""],"execution_count":19,"outputs":[{"output_type":"stream","text":["Traceback (most recent call last):\n","  File \"<string>\", line 1, in <module>\n","IndexError: list index out of range\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ehQUaltfLI_E","executionInfo":{"status":"ok","timestamp":1621508447780,"user_tz":-600,"elapsed":114565,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}},"outputId":"128973c5-35dd-49c0-cc86-03db569c496f"},"source":["!pip install lvis"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Collecting lvis\n","  Downloading https://files.pythonhosted.org/packages/72/b6/1992240ab48310b5360bfdd1d53163f43bb97d90dc5dc723c67d41c38e78/lvis-0.5.3-py3-none-any.whl\n","Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.7/dist-packages (from lvis) (1.19.5)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from lvis) (1.15.0)\n","Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from lvis) (1.3.1)\n","Requirement already satisfied: matplotlib>=3.1.1 in /usr/local/lib/python3.7/dist-packages (from lvis) (3.2.2)\n","Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from lvis) (0.10.0)\n","Requirement already satisfied: Cython>=0.29.12 in /usr/local/lib/python3.7/dist-packages (from lvis) (0.29.23)\n","Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from lvis) (2.8.1)\n","Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.7/dist-packages (from lvis) (4.1.2.30)\n","Requirement already satisfied: pyparsing>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from lvis) (2.4.7)\n","Installing collected packages: lvis\n","Successfully installed lvis-0.5.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VxJvoXF0LOem","executionInfo":{"status":"ok","timestamp":1621513051947,"user_tz":-600,"elapsed":4718729,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}},"outputId":"d4acef66-17f7-480d-8b93-8021dfc42424"},"source":["!python /content/models/research/object_detection/model_main.py \\\n","    --pipeline_config_path={pipeline_fname} \\\n","    --model_dir={model_dir} \\\n","    --alsologtostderr \\\n","    --num_train_steps={num_steps} \\\n","    --num_eval_steps={num_eval_steps}"],"execution_count":21,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n","W0520 11:00:51.299294 139880106583936 model_lib.py:812] Forced number of epochs for all eval validations to be 1.\n","INFO:tensorflow:Maybe overwriting train_steps: 10000\n","I0520 11:00:51.299521 139880106583936 config_util.py:552] Maybe overwriting train_steps: 10000\n","INFO:tensorflow:Maybe overwriting use_bfloat16: False\n","I0520 11:00:51.299632 139880106583936 config_util.py:552] Maybe overwriting use_bfloat16: False\n","INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: 1\n","I0520 11:00:51.299728 139880106583936 config_util.py:552] Maybe overwriting sample_1_of_n_eval_examples: 1\n","INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n","I0520 11:00:51.299816 139880106583936 config_util.py:552] Maybe overwriting eval_num_epochs: 1\n","WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n","W0520 11:00:51.299934 139880106583936 model_lib.py:828] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n","INFO:tensorflow:create_estimator_and_inputs: use_tpu False, export_to_tpu None\n","I0520 11:00:51.300050 139880106583936 model_lib.py:865] create_estimator_and_inputs: use_tpu False, export_to_tpu None\n","INFO:tensorflow:Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f37d95b4550>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","I0520 11:00:51.300590 139880106583936 estimator.py:212] Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f37d95b4550>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n","WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f37d959e950>) includes params argument, but params are not passed to Estimator.\n","W0520 11:00:51.300896 139880106583936 model_fn.py:630] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7f37d959e950>) includes params argument, but params are not passed to Estimator.\n","INFO:tensorflow:Not using Distribute Coordinator.\n","I0520 11:00:51.301368 139880106583936 estimator_training.py:186] Not using Distribute Coordinator.\n","INFO:tensorflow:Running training and evaluation locally (non-distributed).\n","I0520 11:00:51.301571 139880106583936 training.py:612] Running training and evaluation locally (non-distributed).\n","INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n","I0520 11:00:51.301794 139880106583936 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","W0520 11:00:51.306455 139880106583936 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n","INFO:tensorflow:Reading unweighted datasets: ['/content/BloodRBC/data/annotations/train.record']\n","I0520 11:00:51.328199 139880106583936 dataset_builder.py:163] Reading unweighted datasets: ['/content/BloodRBC/data/annotations/train.record']\n","INFO:tensorflow:Reading record datasets for input file: ['/content/BloodRBC/data/annotations/train.record']\n","I0520 11:00:51.328911 139880106583936 dataset_builder.py:80] Reading record datasets for input file: ['/content/BloodRBC/data/annotations/train.record']\n","INFO:tensorflow:Number of filenames to read: 1\n","I0520 11:00:51.329013 139880106583936 dataset_builder.py:81] Number of filenames to read: 1\n","WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n","W0520 11:00:51.329086 139880106583936 dataset_builder.py:88] num_readers has been reduced to 1 to match input file shards.\n","WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n","W0520 11:00:51.333465 139880106583936 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:105: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n","WARNING:tensorflow:From /content/models/research/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map()\n","W0520 11:00:51.350469 139880106583936 deprecation.py:323] From /content/models/research/object_detection/builders/dataset_builder.py:237: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.data.Dataset.map()\n","WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:110: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0520 11:01:03.349959 139880106583936 deprecation.py:323] From /content/models/research/object_detection/inputs.py:110: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:94: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n","W0520 11:01:03.519002 139880106583936 deprecation.py:323] From /content/models/research/object_detection/inputs.py:94: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n","WARNING:tensorflow:From /content/models/research/object_detection/inputs.py:282: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","W0520 11:01:08.976074 139880106583936 deprecation.py:323] From /content/models/research/object_detection/inputs.py:282: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","INFO:tensorflow:Calling model_fn.\n","I0520 11:01:13.762935 139880106583936 estimator.py:1148] Calling model_fn.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:2802: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","W0520 11:01:13.956617 139880106583936 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:2802: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:01:15.388402 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:01:15.540246 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0520 11:01:15.540698 139880106583936 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","WARNING:tensorflow:From /content/models/research/object_detection/utils/spatial_transform_ops.py:478: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n","Instructions for updating:\n","box_ind is deprecated, use box_indices instead\n","W0520 11:01:22.109797 139880106583936 deprecation.py:506] From /content/models/research/object_detection/utils/spatial_transform_ops.py:478: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n","Instructions for updating:\n","box_ind is deprecated, use box_indices instead\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1666: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n","W0520 11:01:22.814904 139880106583936 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1666: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:01:22.817212 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:01:22.833309 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","WARNING:tensorflow:From /content/models/research/object_detection/core/losses.py:453: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n","W0520 11:01:25.421847 139880106583936 deprecation.py:323] From /content/models/research/object_detection/core/losses.py:453: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","\n","Future major versions of TensorFlow will allow gradients to flow\n","into the labels input on backprop by default.\n","\n","See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n","\n","/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","INFO:tensorflow:Done calling model_fn.\n","I0520 11:01:32.491412 139880106583936 estimator.py:1150] Done calling model_fn.\n","INFO:tensorflow:Create CheckpointSaverHook.\n","I0520 11:01:32.492857 139880106583936 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n","INFO:tensorflow:Graph was finalized.\n","I0520 11:01:36.090190 139880106583936 monitored_session.py:240] Graph was finalized.\n","2021-05-20 11:01:36.090654: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","2021-05-20 11:01:36.095395: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n","2021-05-20 11:01:36.095633: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55937227aa80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2021-05-20 11:01:36.095669: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","2021-05-20 11:01:36.118399: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2021-05-20 11:01:36.169341: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","2021-05-20 11:01:36.169399: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (b59efc49287c): /proc/driver/nvidia/version does not exist\n","INFO:tensorflow:Running local_init_op.\n","I0520 11:01:38.892385 139880106583936 session_manager.py:500] Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","I0520 11:01:39.306921 139880106583936 session_manager.py:502] Done running local_init_op.\n","INFO:tensorflow:Saving checkpoints for 0 into training/model.ckpt.\n","I0520 11:01:50.465118 139880106583936 basic_session_run_hooks.py:606] Saving checkpoints for 0 into training/model.ckpt.\n","2021-05-20 11:02:02.653830: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 368640000 exceeds 10% of system memory.\n","2021-05-20 11:02:02.919001: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 368640000 exceeds 10% of system memory.\n","2021-05-20 11:02:14.634643: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 346816512 exceeds 10% of system memory.\n","2021-05-20 11:02:52.559776: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 368640000 exceeds 10% of system memory.\n","2021-05-20 11:02:53.236518: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 368640000 exceeds 10% of system memory.\n","INFO:tensorflow:loss = 3.0748954, step = 1\n","I0520 11:02:55.297797 139880106583936 basic_session_run_hooks.py:262] loss = 3.0748954, step = 1\n","INFO:tensorflow:Saving checkpoints for 12 into training/model.ckpt.\n","I0520 11:12:14.482213 139880106583936 basic_session_run_hooks.py:606] Saving checkpoints for 12 into training/model.ckpt.\n","INFO:tensorflow:Reading unweighted datasets: ['/content/BloodRBC/data/annotations/test.record']\n","I0520 11:12:16.276177 139880106583936 dataset_builder.py:163] Reading unweighted datasets: ['/content/BloodRBC/data/annotations/test.record']\n","INFO:tensorflow:Reading record datasets for input file: ['/content/BloodRBC/data/annotations/test.record']\n","I0520 11:12:16.277038 139880106583936 dataset_builder.py:80] Reading record datasets for input file: ['/content/BloodRBC/data/annotations/test.record']\n","INFO:tensorflow:Number of filenames to read: 1\n","I0520 11:12:16.277201 139880106583936 dataset_builder.py:81] Number of filenames to read: 1\n","INFO:tensorflow:Calling model_fn.\n","I0520 11:12:17.323552 139880106583936 estimator.py:1148] Calling model_fn.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:12:18.692219 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:12:18.840541 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0520 11:12:18.840959 139880106583936 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:12:19.899439 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:12:19.919779 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","WARNING:tensorflow:From /content/models/research/object_detection/eval_util.py:929: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","W0520 11:12:21.103229 139880106583936 deprecation.py:323] From /content/models/research/object_detection/eval_util.py:929: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","WARNING:tensorflow:From /content/models/research/object_detection/utils/visualization_utils.py:618: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","tf.py_func is deprecated in TF V2. Instead, there are two\n","    options available in V2.\n","    - tf.py_function takes a python function which manipulates tf eager\n","    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n","    an ndarray (just call tensor.numpy()) but having access to eager tensors\n","    means `tf.py_function`s can use accelerators such as GPUs as well as\n","    being differentiable using a gradient tape.\n","    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n","    (it is not differentiable, and manipulates numpy arrays). It drops the\n","    stateful argument making all functions stateful.\n","    \n","W0520 11:12:21.293489 139880106583936 deprecation.py:323] From /content/models/research/object_detection/utils/visualization_utils.py:618: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","tf.py_func is deprecated in TF V2. Instead, there are two\n","    options available in V2.\n","    - tf.py_function takes a python function which manipulates tf eager\n","    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n","    an ndarray (just call tensor.numpy()) but having access to eager tensors\n","    means `tf.py_function`s can use accelerators such as GPUs as well as\n","    being differentiable using a gradient tape.\n","    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n","    (it is not differentiable, and manipulates numpy arrays). It drops the\n","    stateful argument making all functions stateful.\n","    \n","INFO:tensorflow:Done calling model_fn.\n","I0520 11:12:21.823445 139880106583936 estimator.py:1150] Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2021-05-20T11:12:21Z\n","I0520 11:12:21.839586 139880106583936 evaluation.py:255] Starting evaluation at 2021-05-20T11:12:21Z\n","INFO:tensorflow:Graph was finalized.\n","I0520 11:12:22.286035 139880106583936 monitored_session.py:240] Graph was finalized.\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-12\n","I0520 11:12:22.287502 139880106583936 saver.py:1284] Restoring parameters from training/model.ckpt-12\n","INFO:tensorflow:Running local_init_op.\n","I0520 11:12:23.272444 139880106583936 session_manager.py:500] Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","I0520 11:12:23.433907 139880106583936 session_manager.py:502] Done running local_init_op.\n","INFO:tensorflow:Performing evaluation on 68 images.\n","I0520 11:14:34.671898 139877632747264 coco_evaluation.py:293] Performing evaluation on 68 images.\n","creating index...\n","index created!\n","INFO:tensorflow:Loading and preparing annotation results...\n","I0520 11:14:34.672876 139877632747264 coco_tools.py:116] Loading and preparing annotation results...\n","INFO:tensorflow:DONE (t=0.00s)\n","I0520 11:14:34.676370 139877632747264 coco_tools.py:138] DONE (t=0.00s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=3.48s).\n","Accumulating evaluation results...\n","DONE (t=0.05s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.004\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.020\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.005\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.004\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.002\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.013\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.116\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.107\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.119\n","INFO:tensorflow:Finished evaluation at 2021-05-20-11:14:38\n","I0520 11:14:38.305104 139880106583936 evaluation.py:275] Finished evaluation at 2021-05-20-11:14:38\n","INFO:tensorflow:Saving dict for global step 12: DetectionBoxes_Precision/mAP = 0.003842557, DetectionBoxes_Precision/mAP (large) = 0.0042295973, DetectionBoxes_Precision/mAP (medium) = 0.0054499465, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.020331623, DetectionBoxes_Precision/mAP@.75IOU = 0.0002955674, DetectionBoxes_Recall/AR@1 = 0.002351314, DetectionBoxes_Recall/AR@10 = 0.013416321, DetectionBoxes_Recall/AR@100 = 0.11618257, DetectionBoxes_Recall/AR@100 (large) = 0.11930147, DetectionBoxes_Recall/AR@100 (medium) = 0.10670391, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/BoxClassifierLoss/classification_loss = 0.19437706, Loss/BoxClassifierLoss/localization_loss = 0.18822305, Loss/RPNLoss/localization_loss = 0.3026791, Loss/RPNLoss/objectness_loss = 0.6840776, Loss/total_loss = 1.3693569, global_step = 12, learning_rate = 0.0002, loss = 1.3693569\n","I0520 11:14:38.305382 139880106583936 estimator.py:2049] Saving dict for global step 12: DetectionBoxes_Precision/mAP = 0.003842557, DetectionBoxes_Precision/mAP (large) = 0.0042295973, DetectionBoxes_Precision/mAP (medium) = 0.0054499465, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.020331623, DetectionBoxes_Precision/mAP@.75IOU = 0.0002955674, DetectionBoxes_Recall/AR@1 = 0.002351314, DetectionBoxes_Recall/AR@10 = 0.013416321, DetectionBoxes_Recall/AR@100 = 0.11618257, DetectionBoxes_Recall/AR@100 (large) = 0.11930147, DetectionBoxes_Recall/AR@100 (medium) = 0.10670391, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/BoxClassifierLoss/classification_loss = 0.19437706, Loss/BoxClassifierLoss/localization_loss = 0.18822305, Loss/RPNLoss/localization_loss = 0.3026791, Loss/RPNLoss/objectness_loss = 0.6840776, Loss/total_loss = 1.3693569, global_step = 12, learning_rate = 0.0002, loss = 1.3693569\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 12: training/model.ckpt-12\n","I0520 11:14:39.234882 139880106583936 estimator.py:2109] Saving 'checkpoint_path' summary for global step 12: training/model.ckpt-12\n","INFO:tensorflow:Saving checkpoints for 22 into training/model.ckpt.\n","I0520 11:23:02.558652 139880106583936 basic_session_run_hooks.py:606] Saving checkpoints for 22 into training/model.ckpt.\n","INFO:tensorflow:Reading unweighted datasets: ['/content/BloodRBC/data/annotations/test.record']\n","I0520 11:23:04.396255 139880106583936 dataset_builder.py:163] Reading unweighted datasets: ['/content/BloodRBC/data/annotations/test.record']\n","INFO:tensorflow:Reading record datasets for input file: ['/content/BloodRBC/data/annotations/test.record']\n","I0520 11:23:04.397051 139880106583936 dataset_builder.py:80] Reading record datasets for input file: ['/content/BloodRBC/data/annotations/test.record']\n","INFO:tensorflow:Number of filenames to read: 1\n","I0520 11:23:04.397167 139880106583936 dataset_builder.py:81] Number of filenames to read: 1\n","INFO:tensorflow:Calling model_fn.\n","I0520 11:23:05.219091 139880106583936 estimator.py:1148] Calling model_fn.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:23:06.615216 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:23:06.755192 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0520 11:23:06.755743 139880106583936 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:23:07.852610 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:23:07.870901 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Done calling model_fn.\n","I0520 11:23:09.629051 139880106583936 estimator.py:1150] Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2021-05-20T11:23:09Z\n","I0520 11:23:09.645990 139880106583936 evaluation.py:255] Starting evaluation at 2021-05-20T11:23:09Z\n","INFO:tensorflow:Graph was finalized.\n","I0520 11:23:10.135267 139880106583936 monitored_session.py:240] Graph was finalized.\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-22\n","I0520 11:23:10.137053 139880106583936 saver.py:1284] Restoring parameters from training/model.ckpt-22\n","INFO:tensorflow:Running local_init_op.\n","I0520 11:23:11.062211 139880106583936 session_manager.py:500] Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","I0520 11:23:11.207683 139880106583936 session_manager.py:502] Done running local_init_op.\n","INFO:tensorflow:Performing evaluation on 68 images.\n","I0520 11:25:21.732619 139877624354560 coco_evaluation.py:293] Performing evaluation on 68 images.\n","creating index...\n","index created!\n","INFO:tensorflow:Loading and preparing annotation results...\n","I0520 11:25:21.733750 139877624354560 coco_tools.py:116] Loading and preparing annotation results...\n","INFO:tensorflow:DONE (t=0.00s)\n","I0520 11:25:21.738896 139877624354560 coco_tools.py:138] DONE (t=0.00s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=3.44s).\n","Accumulating evaluation results...\n","DONE (t=0.06s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.010\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.042\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.012\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.010\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.004\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.023\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.177\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.159\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.183\n","INFO:tensorflow:Finished evaluation at 2021-05-20-11:25:25\n","I0520 11:25:25.320893 139880106583936 evaluation.py:275] Finished evaluation at 2021-05-20-11:25:25\n","INFO:tensorflow:Saving dict for global step 22: DetectionBoxes_Precision/mAP = 0.009670446, DetectionBoxes_Precision/mAP (large) = 0.009971841, DetectionBoxes_Precision/mAP (medium) = 0.012489563, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.042155765, DetectionBoxes_Precision/mAP@.75IOU = 0.0008281362, DetectionBoxes_Recall/AR@1 = 0.0042876904, DetectionBoxes_Recall/AR@10 = 0.023236515, DetectionBoxes_Recall/AR@100 = 0.1769018, DetectionBoxes_Recall/AR@100 (large) = 0.18272059, DetectionBoxes_Recall/AR@100 (medium) = 0.15921788, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/BoxClassifierLoss/classification_loss = 0.3851031, Loss/BoxClassifierLoss/localization_loss = 0.27892724, Loss/RPNLoss/localization_loss = 0.300999, Loss/RPNLoss/objectness_loss = 0.6694262, Loss/total_loss = 1.6344554, global_step = 22, learning_rate = 0.0002, loss = 1.6344554\n","I0520 11:25:25.321157 139880106583936 estimator.py:2049] Saving dict for global step 22: DetectionBoxes_Precision/mAP = 0.009670446, DetectionBoxes_Precision/mAP (large) = 0.009971841, DetectionBoxes_Precision/mAP (medium) = 0.012489563, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.042155765, DetectionBoxes_Precision/mAP@.75IOU = 0.0008281362, DetectionBoxes_Recall/AR@1 = 0.0042876904, DetectionBoxes_Recall/AR@10 = 0.023236515, DetectionBoxes_Recall/AR@100 = 0.1769018, DetectionBoxes_Recall/AR@100 (large) = 0.18272059, DetectionBoxes_Recall/AR@100 (medium) = 0.15921788, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/BoxClassifierLoss/classification_loss = 0.3851031, Loss/BoxClassifierLoss/localization_loss = 0.27892724, Loss/RPNLoss/localization_loss = 0.300999, Loss/RPNLoss/objectness_loss = 0.6694262, Loss/total_loss = 1.6344554, global_step = 22, learning_rate = 0.0002, loss = 1.6344554\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 22: training/model.ckpt-22\n","I0520 11:25:25.326081 139880106583936 estimator.py:2109] Saving 'checkpoint_path' summary for global step 22: training/model.ckpt-22\n","INFO:tensorflow:Saving checkpoints for 32 into training/model.ckpt.\n","I0520 11:33:44.644088 139880106583936 basic_session_run_hooks.py:606] Saving checkpoints for 32 into training/model.ckpt.\n","INFO:tensorflow:Reading unweighted datasets: ['/content/BloodRBC/data/annotations/test.record']\n","I0520 11:33:46.439403 139880106583936 dataset_builder.py:163] Reading unweighted datasets: ['/content/BloodRBC/data/annotations/test.record']\n","INFO:tensorflow:Reading record datasets for input file: ['/content/BloodRBC/data/annotations/test.record']\n","I0520 11:33:46.440374 139880106583936 dataset_builder.py:80] Reading record datasets for input file: ['/content/BloodRBC/data/annotations/test.record']\n","INFO:tensorflow:Number of filenames to read: 1\n","I0520 11:33:46.440559 139880106583936 dataset_builder.py:81] Number of filenames to read: 1\n","INFO:tensorflow:Calling model_fn.\n","I0520 11:33:47.237216 139880106583936 estimator.py:1148] Calling model_fn.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:33:48.582896 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:33:48.717880 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0520 11:33:48.718273 139880106583936 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:33:49.718805 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:33:49.742602 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Done calling model_fn.\n","I0520 11:33:51.415558 139880106583936 estimator.py:1150] Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2021-05-20T11:33:51Z\n","I0520 11:33:51.433342 139880106583936 evaluation.py:255] Starting evaluation at 2021-05-20T11:33:51Z\n","INFO:tensorflow:Graph was finalized.\n","I0520 11:33:51.887594 139880106583936 monitored_session.py:240] Graph was finalized.\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-32\n","I0520 11:33:51.889063 139880106583936 saver.py:1284] Restoring parameters from training/model.ckpt-32\n","INFO:tensorflow:Running local_init_op.\n","I0520 11:33:52.785737 139880106583936 session_manager.py:500] Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","I0520 11:33:52.936677 139880106583936 session_manager.py:502] Done running local_init_op.\n","INFO:tensorflow:Performing evaluation on 68 images.\n","I0520 11:36:03.307676 139877624354560 coco_evaluation.py:293] Performing evaluation on 68 images.\n","creating index...\n","index created!\n","INFO:tensorflow:Loading and preparing annotation results...\n","I0520 11:36:03.308793 139877624354560 coco_tools.py:116] Loading and preparing annotation results...\n","INFO:tensorflow:DONE (t=0.01s)\n","I0520 11:36:03.315108 139877624354560 coco_tools.py:138] DONE (t=0.01s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=3.38s).\n","Accumulating evaluation results...\n","DONE (t=0.05s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.021\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.086\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.002\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.022\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.023\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.003\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.062\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.209\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.141\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.232\n","INFO:tensorflow:Finished evaluation at 2021-05-20-11:36:06\n","I0520 11:36:06.817615 139880106583936 evaluation.py:275] Finished evaluation at 2021-05-20-11:36:06\n","INFO:tensorflow:Saving dict for global step 32: DetectionBoxes_Precision/mAP = 0.021425044, DetectionBoxes_Precision/mAP (large) = 0.02268945, DetectionBoxes_Precision/mAP (medium) = 0.02213592, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.08572994, DetectionBoxes_Precision/mAP@.75IOU = 0.0023121622, DetectionBoxes_Recall/AR@1 = 0.003042877, DetectionBoxes_Recall/AR@10 = 0.06210235, DetectionBoxes_Recall/AR@100 = 0.20912863, DetectionBoxes_Recall/AR@100 (large) = 0.23161764, DetectionBoxes_Recall/AR@100 (medium) = 0.14078212, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/BoxClassifierLoss/classification_loss = 0.3199247, Loss/BoxClassifierLoss/localization_loss = 0.28416136, Loss/RPNLoss/localization_loss = 0.3003382, Loss/RPNLoss/objectness_loss = 0.65296215, Loss/total_loss = 1.5573864, global_step = 32, learning_rate = 0.0002, loss = 1.5573864\n","I0520 11:36:06.817875 139880106583936 estimator.py:2049] Saving dict for global step 32: DetectionBoxes_Precision/mAP = 0.021425044, DetectionBoxes_Precision/mAP (large) = 0.02268945, DetectionBoxes_Precision/mAP (medium) = 0.02213592, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.08572994, DetectionBoxes_Precision/mAP@.75IOU = 0.0023121622, DetectionBoxes_Recall/AR@1 = 0.003042877, DetectionBoxes_Recall/AR@10 = 0.06210235, DetectionBoxes_Recall/AR@100 = 0.20912863, DetectionBoxes_Recall/AR@100 (large) = 0.23161764, DetectionBoxes_Recall/AR@100 (medium) = 0.14078212, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/BoxClassifierLoss/classification_loss = 0.3199247, Loss/BoxClassifierLoss/localization_loss = 0.28416136, Loss/RPNLoss/localization_loss = 0.3003382, Loss/RPNLoss/objectness_loss = 0.65296215, Loss/total_loss = 1.5573864, global_step = 32, learning_rate = 0.0002, loss = 1.5573864\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 32: training/model.ckpt-32\n","I0520 11:36:06.822776 139880106583936 estimator.py:2109] Saving 'checkpoint_path' summary for global step 32: training/model.ckpt-32\n","INFO:tensorflow:Saving checkpoints for 42 into training/model.ckpt.\n","I0520 11:44:26.891252 139880106583936 basic_session_run_hooks.py:606] Saving checkpoints for 42 into training/model.ckpt.\n","INFO:tensorflow:Reading unweighted datasets: ['/content/BloodRBC/data/annotations/test.record']\n","I0520 11:44:28.639147 139880106583936 dataset_builder.py:163] Reading unweighted datasets: ['/content/BloodRBC/data/annotations/test.record']\n","INFO:tensorflow:Reading record datasets for input file: ['/content/BloodRBC/data/annotations/test.record']\n","I0520 11:44:28.641242 139880106583936 dataset_builder.py:80] Reading record datasets for input file: ['/content/BloodRBC/data/annotations/test.record']\n","INFO:tensorflow:Number of filenames to read: 1\n","I0520 11:44:28.641434 139880106583936 dataset_builder.py:81] Number of filenames to read: 1\n","INFO:tensorflow:Calling model_fn.\n","I0520 11:44:29.446266 139880106583936 estimator.py:1148] Calling model_fn.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:44:30.797879 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:44:30.939621 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0520 11:44:30.940035 139880106583936 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:44:31.943810 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:44:31.959879 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Done calling model_fn.\n","I0520 11:44:34.148052 139880106583936 estimator.py:1150] Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2021-05-20T11:44:34Z\n","I0520 11:44:34.165132 139880106583936 evaluation.py:255] Starting evaluation at 2021-05-20T11:44:34Z\n","INFO:tensorflow:Graph was finalized.\n","I0520 11:44:34.637558 139880106583936 monitored_session.py:240] Graph was finalized.\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-42\n","I0520 11:44:34.639182 139880106583936 saver.py:1284] Restoring parameters from training/model.ckpt-42\n","INFO:tensorflow:Running local_init_op.\n","I0520 11:44:35.636441 139880106583936 session_manager.py:500] Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","I0520 11:44:35.809995 139880106583936 session_manager.py:502] Done running local_init_op.\n","INFO:tensorflow:Performing evaluation on 68 images.\n","I0520 11:46:47.813338 139877624354560 coco_evaluation.py:293] Performing evaluation on 68 images.\n","creating index...\n","index created!\n","INFO:tensorflow:Loading and preparing annotation results...\n","I0520 11:46:47.814682 139877624354560 coco_tools.py:116] Loading and preparing annotation results...\n","INFO:tensorflow:DONE (t=0.01s)\n","I0520 11:46:47.820112 139877624354560 coco_tools.py:138] DONE (t=0.01s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=3.47s).\n","Accumulating evaluation results...\n","DONE (t=0.05s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.026\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.102\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.003\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.043\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.025\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.008\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.068\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.212\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.180\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.223\n","INFO:tensorflow:Finished evaluation at 2021-05-20-11:46:51\n","I0520 11:46:51.431376 139880106583936 evaluation.py:275] Finished evaluation at 2021-05-20-11:46:51\n","INFO:tensorflow:Saving dict for global step 42: DetectionBoxes_Precision/mAP = 0.025712477, DetectionBoxes_Precision/mAP (large) = 0.025226137, DetectionBoxes_Precision/mAP (medium) = 0.042657703, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.10198877, DetectionBoxes_Precision/mAP@.75IOU = 0.0030614343, DetectionBoxes_Recall/AR@1 = 0.008298756, DetectionBoxes_Recall/AR@10 = 0.06818811, DetectionBoxes_Recall/AR@100 = 0.21217151, DetectionBoxes_Recall/AR@100 (large) = 0.2226103, DetectionBoxes_Recall/AR@100 (medium) = 0.18044692, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/BoxClassifierLoss/classification_loss = 0.34933668, Loss/BoxClassifierLoss/localization_loss = 0.309112, Loss/RPNLoss/localization_loss = 0.29952604, Loss/RPNLoss/objectness_loss = 0.63006735, Loss/total_loss = 1.5880423, global_step = 42, learning_rate = 0.0002, loss = 1.5880423\n","I0520 11:46:51.431707 139880106583936 estimator.py:2049] Saving dict for global step 42: DetectionBoxes_Precision/mAP = 0.025712477, DetectionBoxes_Precision/mAP (large) = 0.025226137, DetectionBoxes_Precision/mAP (medium) = 0.042657703, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.10198877, DetectionBoxes_Precision/mAP@.75IOU = 0.0030614343, DetectionBoxes_Recall/AR@1 = 0.008298756, DetectionBoxes_Recall/AR@10 = 0.06818811, DetectionBoxes_Recall/AR@100 = 0.21217151, DetectionBoxes_Recall/AR@100 (large) = 0.2226103, DetectionBoxes_Recall/AR@100 (medium) = 0.18044692, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/BoxClassifierLoss/classification_loss = 0.34933668, Loss/BoxClassifierLoss/localization_loss = 0.309112, Loss/RPNLoss/localization_loss = 0.29952604, Loss/RPNLoss/objectness_loss = 0.63006735, Loss/total_loss = 1.5880423, global_step = 42, learning_rate = 0.0002, loss = 1.5880423\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 42: training/model.ckpt-42\n","I0520 11:46:51.439824 139880106583936 estimator.py:2109] Saving 'checkpoint_path' summary for global step 42: training/model.ckpt-42\n","INFO:tensorflow:Saving checkpoints for 52 into training/model.ckpt.\n","I0520 11:55:14.463603 139880106583936 basic_session_run_hooks.py:606] Saving checkpoints for 52 into training/model.ckpt.\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","W0520 11:55:14.580321 139880106583936 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to delete files with this prefix.\n","INFO:tensorflow:Reading unweighted datasets: ['/content/BloodRBC/data/annotations/test.record']\n","I0520 11:55:16.230316 139880106583936 dataset_builder.py:163] Reading unweighted datasets: ['/content/BloodRBC/data/annotations/test.record']\n","INFO:tensorflow:Reading record datasets for input file: ['/content/BloodRBC/data/annotations/test.record']\n","I0520 11:55:16.231559 139880106583936 dataset_builder.py:80] Reading record datasets for input file: ['/content/BloodRBC/data/annotations/test.record']\n","INFO:tensorflow:Number of filenames to read: 1\n","I0520 11:55:16.231710 139880106583936 dataset_builder.py:81] Number of filenames to read: 1\n","INFO:tensorflow:Calling model_fn.\n","I0520 11:55:17.020368 139880106583936 estimator.py:1148] Calling model_fn.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:55:18.327529 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:55:18.459442 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0520 11:55:18.459851 139880106583936 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:55:19.470195 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 11:55:19.486909 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Done calling model_fn.\n","I0520 11:55:21.203524 139880106583936 estimator.py:1150] Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2021-05-20T11:55:21Z\n","I0520 11:55:21.220234 139880106583936 evaluation.py:255] Starting evaluation at 2021-05-20T11:55:21Z\n","INFO:tensorflow:Graph was finalized.\n","I0520 11:55:21.666129 139880106583936 monitored_session.py:240] Graph was finalized.\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-52\n","I0520 11:55:21.667680 139880106583936 saver.py:1284] Restoring parameters from training/model.ckpt-52\n","INFO:tensorflow:Running local_init_op.\n","I0520 11:55:22.641269 139880106583936 session_manager.py:500] Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","I0520 11:55:22.810250 139880106583936 session_manager.py:502] Done running local_init_op.\n","INFO:tensorflow:Performing evaluation on 68 images.\n","I0520 11:57:31.779785 139877632747264 coco_evaluation.py:293] Performing evaluation on 68 images.\n","creating index...\n","index created!\n","INFO:tensorflow:Loading and preparing annotation results...\n","I0520 11:57:31.780856 139877632747264 coco_tools.py:116] Loading and preparing annotation results...\n","INFO:tensorflow:DONE (t=0.00s)\n","I0520 11:57:31.784347 139877632747264 coco_tools.py:138] DONE (t=0.00s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=3.38s).\n","Accumulating evaluation results...\n","DONE (t=0.05s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.028\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.104\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.006\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.058\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.027\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.012\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.084\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.216\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.165\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.233\n","INFO:tensorflow:Finished evaluation at 2021-05-20-11:57:35\n","I0520 11:57:35.291777 139880106583936 evaluation.py:275] Finished evaluation at 2021-05-20-11:57:35\n","INFO:tensorflow:Saving dict for global step 52: DetectionBoxes_Precision/mAP = 0.02805913, DetectionBoxes_Precision/mAP (large) = 0.027228368, DetectionBoxes_Precision/mAP (medium) = 0.05798836, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.104275815, DetectionBoxes_Precision/mAP@.75IOU = 0.005914399, DetectionBoxes_Recall/AR@1 = 0.0116182575, DetectionBoxes_Recall/AR@10 = 0.083540805, DetectionBoxes_Recall/AR@100 = 0.21618257, DetectionBoxes_Recall/AR@100 (large) = 0.2329044, DetectionBoxes_Recall/AR@100 (medium) = 0.16536313, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/BoxClassifierLoss/classification_loss = 0.33021298, Loss/BoxClassifierLoss/localization_loss = 0.31602108, Loss/RPNLoss/localization_loss = 0.29834178, Loss/RPNLoss/objectness_loss = 0.5999657, Loss/total_loss = 1.5445415, global_step = 52, learning_rate = 0.0002, loss = 1.5445415\n","I0520 11:57:35.292055 139880106583936 estimator.py:2049] Saving dict for global step 52: DetectionBoxes_Precision/mAP = 0.02805913, DetectionBoxes_Precision/mAP (large) = 0.027228368, DetectionBoxes_Precision/mAP (medium) = 0.05798836, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.104275815, DetectionBoxes_Precision/mAP@.75IOU = 0.005914399, DetectionBoxes_Recall/AR@1 = 0.0116182575, DetectionBoxes_Recall/AR@10 = 0.083540805, DetectionBoxes_Recall/AR@100 = 0.21618257, DetectionBoxes_Recall/AR@100 (large) = 0.2329044, DetectionBoxes_Recall/AR@100 (medium) = 0.16536313, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/BoxClassifierLoss/classification_loss = 0.33021298, Loss/BoxClassifierLoss/localization_loss = 0.31602108, Loss/RPNLoss/localization_loss = 0.29834178, Loss/RPNLoss/objectness_loss = 0.5999657, Loss/total_loss = 1.5445415, global_step = 52, learning_rate = 0.0002, loss = 1.5445415\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 52: training/model.ckpt-52\n","I0520 11:57:35.296877 139880106583936 estimator.py:2109] Saving 'checkpoint_path' summary for global step 52: training/model.ckpt-52\n","INFO:tensorflow:Saving checkpoints for 62 into training/model.ckpt.\n","I0520 12:05:56.104657 139880106583936 basic_session_run_hooks.py:606] Saving checkpoints for 62 into training/model.ckpt.\n","INFO:tensorflow:Reading unweighted datasets: ['/content/BloodRBC/data/annotations/test.record']\n","I0520 12:05:57.856180 139880106583936 dataset_builder.py:163] Reading unweighted datasets: ['/content/BloodRBC/data/annotations/test.record']\n","INFO:tensorflow:Reading record datasets for input file: ['/content/BloodRBC/data/annotations/test.record']\n","I0520 12:05:57.857092 139880106583936 dataset_builder.py:80] Reading record datasets for input file: ['/content/BloodRBC/data/annotations/test.record']\n","INFO:tensorflow:Number of filenames to read: 1\n","I0520 12:05:57.857237 139880106583936 dataset_builder.py:81] Number of filenames to read: 1\n","INFO:tensorflow:Calling model_fn.\n","I0520 12:05:58.652262 139880106583936 estimator.py:1148] Calling model_fn.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 12:05:59.993195 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 12:06:00.138037 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0520 12:06:00.138536 139880106583936 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 12:06:01.171049 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 12:06:01.189180 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Done calling model_fn.\n","I0520 12:06:03.278626 139880106583936 estimator.py:1150] Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2021-05-20T12:06:03Z\n","I0520 12:06:03.297009 139880106583936 evaluation.py:255] Starting evaluation at 2021-05-20T12:06:03Z\n","INFO:tensorflow:Graph was finalized.\n","I0520 12:06:03.764412 139880106583936 monitored_session.py:240] Graph was finalized.\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-62\n","I0520 12:06:03.766216 139880106583936 saver.py:1284] Restoring parameters from training/model.ckpt-62\n","INFO:tensorflow:Running local_init_op.\n","I0520 12:06:04.748316 139880106583936 session_manager.py:500] Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","I0520 12:06:04.907317 139880106583936 session_manager.py:502] Done running local_init_op.\n","INFO:tensorflow:Performing evaluation on 68 images.\n","I0520 12:08:15.459656 139877624354560 coco_evaluation.py:293] Performing evaluation on 68 images.\n","creating index...\n","index created!\n","INFO:tensorflow:Loading and preparing annotation results...\n","I0520 12:08:15.460452 139877624354560 coco_tools.py:116] Loading and preparing annotation results...\n","INFO:tensorflow:DONE (t=0.00s)\n","I0520 12:08:15.463892 139877624354560 coco_tools.py:138] DONE (t=0.00s)\n","creating index...\n","index created!\n","Running per image evaluation...\n","Evaluate annotation type *bbox*\n","DONE (t=3.43s).\n","Accumulating evaluation results...\n","DONE (t=0.05s).\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.027\n"," Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.101\n"," Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.004\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.040\n"," Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.026\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.013\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.070\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.203\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.166\n"," Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.216\n","INFO:tensorflow:Finished evaluation at 2021-05-20-12:08:19\n","I0520 12:08:19.032078 139880106583936 evaluation.py:275] Finished evaluation at 2021-05-20-12:08:19\n","INFO:tensorflow:Saving dict for global step 62: DetectionBoxes_Precision/mAP = 0.026751548, DetectionBoxes_Precision/mAP (large) = 0.026238749, DetectionBoxes_Precision/mAP (medium) = 0.03953152, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.100670055, DetectionBoxes_Precision/mAP@.75IOU = 0.0038969526, DetectionBoxes_Recall/AR@1 = 0.012586446, DetectionBoxes_Recall/AR@10 = 0.06970955, DetectionBoxes_Recall/AR@100 = 0.20345782, DetectionBoxes_Recall/AR@100 (large) = 0.215625, DetectionBoxes_Recall/AR@100 (medium) = 0.16648045, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/BoxClassifierLoss/classification_loss = 0.34024036, Loss/BoxClassifierLoss/localization_loss = 0.30307308, Loss/RPNLoss/localization_loss = 0.29715642, Loss/RPNLoss/objectness_loss = 0.5577341, Loss/total_loss = 1.4982039, global_step = 62, learning_rate = 0.0002, loss = 1.4982039\n","I0520 12:08:19.032339 139880106583936 estimator.py:2049] Saving dict for global step 62: DetectionBoxes_Precision/mAP = 0.026751548, DetectionBoxes_Precision/mAP (large) = 0.026238749, DetectionBoxes_Precision/mAP (medium) = 0.03953152, DetectionBoxes_Precision/mAP (small) = -1.0, DetectionBoxes_Precision/mAP@.50IOU = 0.100670055, DetectionBoxes_Precision/mAP@.75IOU = 0.0038969526, DetectionBoxes_Recall/AR@1 = 0.012586446, DetectionBoxes_Recall/AR@10 = 0.06970955, DetectionBoxes_Recall/AR@100 = 0.20345782, DetectionBoxes_Recall/AR@100 (large) = 0.215625, DetectionBoxes_Recall/AR@100 (medium) = 0.16648045, DetectionBoxes_Recall/AR@100 (small) = -1.0, Loss/BoxClassifierLoss/classification_loss = 0.34024036, Loss/BoxClassifierLoss/localization_loss = 0.30307308, Loss/RPNLoss/localization_loss = 0.29715642, Loss/RPNLoss/objectness_loss = 0.5577341, Loss/total_loss = 1.4982039, global_step = 62, learning_rate = 0.0002, loss = 1.4982039\n","INFO:tensorflow:Saving 'checkpoint_path' summary for global step 62: training/model.ckpt-62\n","I0520 12:08:19.037716 139880106583936 estimator.py:2109] Saving 'checkpoint_path' summary for global step 62: training/model.ckpt-62\n","INFO:tensorflow:Saving checkpoints for 72 into training/model.ckpt.\n","I0520 12:16:40.591175 139880106583936 basic_session_run_hooks.py:606] Saving checkpoints for 72 into training/model.ckpt.\n","INFO:tensorflow:Reading unweighted datasets: ['/content/BloodRBC/data/annotations/test.record']\n","I0520 12:16:42.370090 139880106583936 dataset_builder.py:163] Reading unweighted datasets: ['/content/BloodRBC/data/annotations/test.record']\n","INFO:tensorflow:Reading record datasets for input file: ['/content/BloodRBC/data/annotations/test.record']\n","I0520 12:16:42.371087 139880106583936 dataset_builder.py:80] Reading record datasets for input file: ['/content/BloodRBC/data/annotations/test.record']\n","INFO:tensorflow:Number of filenames to read: 1\n","I0520 12:16:42.371225 139880106583936 dataset_builder.py:81] Number of filenames to read: 1\n","INFO:tensorflow:Calling model_fn.\n","I0520 12:16:43.177055 139880106583936 estimator.py:1148] Calling model_fn.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 12:16:44.535603 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 12:16:44.666735 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0520 12:16:44.667110 139880106583936 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 12:16:45.635092 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 12:16:45.651545 139880106583936 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Done calling model_fn.\n","I0520 12:16:47.255283 139880106583936 estimator.py:1150] Done calling model_fn.\n","INFO:tensorflow:Starting evaluation at 2021-05-20T12:16:47Z\n","I0520 12:16:47.270774 139880106583936 evaluation.py:255] Starting evaluation at 2021-05-20T12:16:47Z\n","INFO:tensorflow:Graph was finalized.\n","I0520 12:16:47.713835 139880106583936 monitored_session.py:240] Graph was finalized.\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-72\n","I0520 12:16:47.715410 139880106583936 saver.py:1284] Restoring parameters from training/model.ckpt-72\n","INFO:tensorflow:Running local_init_op.\n","I0520 12:16:48.649003 139880106583936 session_manager.py:500] Running local_init_op.\n","INFO:tensorflow:Done running local_init_op.\n","I0520 12:16:48.818704 139880106583936 session_manager.py:502] Done running local_init_op.\n","Traceback (most recent call last):\n","  File \"/content/models/research/object_detection/model_main.py\", line 108, in <module>\n","    tf.app.run()\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n","    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n","  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 303, in run\n","    _run_main(main, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/absl/app.py\", line 251, in _run_main\n","    sys.exit(main(argv))\n","  File \"/content/models/research/object_detection/model_main.py\", line 104, in main\n","    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\n","    return executor.run()\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\n","    return self.run_local()\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\n","    saving_listeners=saving_listeners)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 370, in train\n","    loss = self._train_model(input_fn, hooks, saving_listeners)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1161, in _train_model\n","    return self._train_model_default(input_fn, hooks, saving_listeners)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1195, in _train_model_default\n","    saving_listeners)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1494, in _train_with_estimator_spec\n","    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 754, in run\n","    run_metadata=run_metadata)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1259, in run\n","    run_metadata=run_metadata)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1345, in run\n","    return self._sess.run(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1426, in run\n","    run_metadata=run_metadata))\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/basic_session_run_hooks.py\", line 594, in after_run\n","    if self._save(run_context.session, global_step):\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/basic_session_run_hooks.py\", line 619, in _save\n","    if l.after_save(session, step):\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 519, in after_save\n","    self._evaluate(global_step_value)  # updates self.eval_result\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 539, in _evaluate\n","    self._evaluator.evaluate_and_export())\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 920, in evaluate_and_export\n","    hooks=self._eval_spec.hooks)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 480, in evaluate\n","    name=name)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 522, in _actual_eval\n","    return _evaluate()\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 511, in _evaluate\n","    output_dir=self.eval_dir(name))\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1619, in _evaluate_run\n","    config=self._session_config)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/evaluation.py\", line 272, in _evaluate_once\n","    session.run(eval_ops, feed_dict)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 754, in run\n","    run_metadata=run_metadata)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1259, in run\n","    run_metadata=run_metadata)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1345, in run\n","    return self._sess.run(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1418, in run\n","    run_metadata=run_metadata)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1176, in run\n","    return self._sess.run(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\", line 956, in run\n","    run_metadata_ptr)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\", line 1180, in _run\n","    feed_dict_tensor, options, run_metadata)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n","    run_metadata)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n","    return fn(*args)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n","    target_list, run_metadata)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n","    run_metadata)\n","KeyboardInterrupt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Af09Tvp3LWLt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621513051947,"user_tz":-600,"elapsed":4718727,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}},"outputId":"05665d34-7ddb-4309-b3cb-525a3bacb84c"},"source":["!ls {model_dir}"],"execution_count":22,"outputs":[{"output_type":"stream","text":["checkpoint\t\t\t\t     model.ckpt-52.data-00000-of-00001\n","eval_0\t\t\t\t\t     model.ckpt-52.index\n","events.out.tfevents.1621508494.b59efc49287c  model.ckpt-52.meta\n","graph.pbtxt\t\t\t\t     model.ckpt-62.data-00000-of-00001\n","model.ckpt-32.data-00000-of-00001\t     model.ckpt-62.index\n","model.ckpt-32.index\t\t\t     model.ckpt-62.meta\n","model.ckpt-32.meta\t\t\t     model.ckpt-72.data-00000-of-00001\n","model.ckpt-42.data-00000-of-00001\t     model.ckpt-72.index\n","model.ckpt-42.index\t\t\t     model.ckpt-72.meta\n","model.ckpt-42.meta\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BVYesNYYV48Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621513065786,"user_tz":-600,"elapsed":4732565,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}},"outputId":"37433a46-3cfa-4ebc-e683-e041a8635b7a"},"source":["import re\n","import numpy as np\n","\n","output_directory = './fine_tuned_model'\n","\n","lst = os.listdir(model_dir)\n","lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n","steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n","last_model = lst[steps.argmax()].replace('.meta', '')\n","\n","last_model_path = os.path.join(model_dir, last_model)\n","print(last_model_path)\n","!python /content/models/research/object_detection/export_inference_graph.py \\\n","    --input_type=image_tensor \\\n","    --pipeline_config_path={pipeline_fname} \\\n","    --output_directory={output_directory} \\\n","    --trained_checkpoint_prefix={last_model_path}"],"execution_count":23,"outputs":[{"output_type":"stream","text":["training/model.ckpt-72\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:2802: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","W0520 12:17:35.332445 140686366287744 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:2802: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `layer.__call__` method instead.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 12:17:36.763640 140686366287744 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 12:17:36.901199 140686366287744 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:depth of additional conv before box predictor: 0\n","I0520 12:17:36.901656 140686366287744 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n","WARNING:tensorflow:From /content/models/research/object_detection/core/box_list_ops.py:169: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","W0520 12:17:36.960663 140686366287744 deprecation.py:323] From /content/models/research/object_detection/core/box_list_ops.py:169: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/models/research/object_detection/utils/spatial_transform_ops.py:478: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n","Instructions for updating:\n","box_ind is deprecated, use box_indices instead\n","W0520 12:17:37.510424 140686366287744 deprecation.py:506] From /content/models/research/object_detection/utils/spatial_transform_ops.py:478: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n","Instructions for updating:\n","box_ind is deprecated, use box_indices instead\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1666: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n","W0520 12:17:38.029871 140686366287744 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1666: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use keras.layers.flatten instead.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 12:17:38.036254 140686366287744 regularizers.py:99] Scale of 0 disables regularizer.\n","INFO:tensorflow:Scale of 0 disables regularizer.\n","I0520 12:17:38.056559 140686366287744 regularizers.py:99] Scale of 0 disables regularizer.\n","WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:474: get_or_create_global_step (from tf_slim.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please switch to tf.train.get_or_create_global_step\n","W0520 12:17:38.689167 140686366287744 deprecation.py:323] From /content/models/research/object_detection/exporter.py:474: get_or_create_global_step (from tf_slim.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please switch to tf.train.get_or_create_global_step\n","WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:653: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n","Instructions for updating:\n","Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n","W0520 12:17:38.692264 140686366287744 deprecation.py:323] From /content/models/research/object_detection/exporter.py:653: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n","Instructions for updating:\n","Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n","W0520 12:17:38.693419 140686366287744 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n","204 ops no flops stats due to incomplete shapes.\n","Parsing Inputs...\n","Incomplete shape.\n","\n","=========================Options=============================\n","-max_depth                  10000\n","-min_bytes                  0\n","-min_peak_bytes             0\n","-min_residual_bytes         0\n","-min_output_bytes           0\n","-min_micros                 0\n","-min_accelerator_micros     0\n","-min_cpu_micros             0\n","-min_params                 0\n","-min_float_ops              0\n","-min_occurrence             0\n","-step                       -1\n","-order_by                   name\n","-account_type_regexes       _trainable_variables\n","-start_name_regexes         .*\n","-trim_name_regexes          .*BatchNorm.*\n","-show_name_regexes          .*\n","-hide_name_regexes          \n","-account_displayed_op_only  true\n","-select                     params\n","-output                     stdout:\n","\n","==================Model Analysis Report======================\n","Incomplete shape.\n","\n","Doc:\n","scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n","param: Number of parameters (in the Variable).\n","\n","Profile:\n","node name | # parameters\n","_TFProfRoot (--/12.84m params)\n","  Conv (--/2.65m params)\n","    Conv/biases (512, 512/512 params)\n","    Conv/weights (3x3x576x512, 2.65m/2.65m params)\n","  FirstStageBoxPredictor (--/36.94k params)\n","    FirstStageBoxPredictor/BoxEncodingPredictor (--/24.62k params)\n","      FirstStageBoxPredictor/BoxEncodingPredictor/biases (48, 48/48 params)\n","      FirstStageBoxPredictor/BoxEncodingPredictor/weights (1x1x512x48, 24.58k/24.58k params)\n","    FirstStageBoxPredictor/ClassPredictor (--/12.31k params)\n","      FirstStageBoxPredictor/ClassPredictor/biases (24, 24/24 params)\n","      FirstStageBoxPredictor/ClassPredictor/weights (1x1x512x24, 12.29k/12.29k params)\n","  FirstStageFeatureExtractor (--/4.25m params)\n","    FirstStageFeatureExtractor/InceptionV2 (--/4.25m params)\n","      FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7 (--/2.71k params)\n","        FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/BatchNorm (--/0 params)\n","        FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/depthwise_weights (7x7x3x8, 1.18k/1.18k params)\n","        FirstStageFeatureExtractor/InceptionV2/Conv2d_1a_7x7/pointwise_weights (1x1x24x64, 1.54k/1.54k params)\n","      FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1 (--/4.10k params)\n","        FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1/BatchNorm (--/0 params)\n","        FirstStageFeatureExtractor/InceptionV2/Conv2d_2b_1x1/weights (1x1x64x64, 4.10k/4.10k params)\n","      FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3 (--/110.59k params)\n","        FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/BatchNorm (--/0 params)\n","        FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights (3x3x64x192, 110.59k/110.59k params)\n","      FirstStageFeatureExtractor/InceptionV2/Mixed_3b (--/218.11k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0 (--/12.29k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1 (--/12.29k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_0/Conv2d_0a_1x1/weights (1x1x192x64, 12.29k/12.29k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1 (--/49.15k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1 (--/12.29k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0a_1x1/weights (1x1x192x64, 12.29k/12.29k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3 (--/36.86k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_1/Conv2d_0b_3x3/weights (3x3x64x64, 36.86k/36.86k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2 (--/150.53k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1 (--/12.29k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0a_1x1/weights (1x1x192x64, 12.29k/12.29k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3 (--/82.94k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_2/Conv2d_0c_3x3/weights (3x3x96x96, 82.94k/82.94k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3 (--/6.14k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1 (--/6.14k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3b/Branch_3/Conv2d_0b_1x1/weights (1x1x192x32, 6.14k/6.14k params)\n","      FirstStageFeatureExtractor/InceptionV2/Mixed_3c (--/259.07k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0 (--/16.38k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1 (--/16.38k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_0/Conv2d_0a_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1 (--/71.68k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1 (--/16.38k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0a_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_1/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2 (--/154.62k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1 (--/16.38k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0a_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3 (--/82.94k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_2/Conv2d_0c_3x3/weights (3x3x96x96, 82.94k/82.94k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3 (--/16.38k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1 (--/16.38k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_3c/Branch_3/Conv2d_0b_1x1/weights (1x1x256x64, 16.38k/16.38k params)\n","      FirstStageFeatureExtractor/InceptionV2/Mixed_4a (--/384.00k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0 (--/225.28k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1 (--/40.96k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_0a_1x1/weights (1x1x320x128, 40.96k/40.96k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3 (--/184.32k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_0/Conv2d_1a_3x3/weights (3x3x128x160, 184.32k/184.32k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1 (--/158.72k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1 (--/20.48k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0a_1x1/weights (1x1x320x64, 20.48k/20.48k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3 (--/82.94k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4a/Branch_1/Conv2d_1a_3x3/weights (3x3x96x96, 82.94k/82.94k params)\n","      FirstStageFeatureExtractor/InceptionV2/Mixed_4b (--/608.26k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0 (--/129.02k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1 (--/129.02k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_0/Conv2d_0a_1x1/weights (1x1x576x224, 129.02k/129.02k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1 (--/92.16k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1 (--/36.86k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0a_1x1/weights (1x1x576x64, 36.86k/36.86k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_1/Conv2d_0b_3x3/weights (3x3x64x96, 55.30k/55.30k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2 (--/313.34k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3 (--/110.59k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0b_3x3/weights (3x3x96x128, 110.59k/110.59k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3 (--/147.46k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_2/Conv2d_0c_3x3/weights (3x3x128x128, 147.46k/147.46k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3 (--/73.73k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1 (--/73.73k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4b/Branch_3/Conv2d_0b_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n","      FirstStageFeatureExtractor/InceptionV2/Mixed_4c (--/663.55k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0 (--/110.59k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1 (--/110.59k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_0/Conv2d_0a_1x1/weights (1x1x576x192, 110.59k/110.59k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1 (--/165.89k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3 (--/110.59k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_1/Conv2d_0b_3x3/weights (3x3x96x128, 110.59k/110.59k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2 (--/313.34k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3 (--/110.59k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0b_3x3/weights (3x3x96x128, 110.59k/110.59k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3 (--/147.46k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_2/Conv2d_0c_3x3/weights (3x3x128x128, 147.46k/147.46k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3 (--/73.73k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1 (--/73.73k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4c/Branch_3/Conv2d_0b_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n","      FirstStageFeatureExtractor/InceptionV2/Mixed_4d (--/893.95k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0 (--/92.16k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1 (--/92.16k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_0/Conv2d_0a_1x1/weights (1x1x576x160, 92.16k/92.16k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1 (--/258.05k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1 (--/73.73k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3 (--/184.32k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_1/Conv2d_0b_3x3/weights (3x3x128x160, 184.32k/184.32k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2 (--/488.45k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1 (--/73.73k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3 (--/184.32k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0b_3x3/weights (3x3x128x160, 184.32k/184.32k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3 (--/230.40k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_2/Conv2d_0c_3x3/weights (3x3x160x160, 230.40k/230.40k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3 (--/55.30k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4d/Branch_3/Conv2d_0b_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n","      FirstStageFeatureExtractor/InceptionV2/Mixed_4e (--/1.11m params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0 (--/55.30k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_0/Conv2d_0a_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1 (--/294.91k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1 (--/73.73k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3 (--/221.18k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_1/Conv2d_0b_3x3/weights (3x3x128x192, 221.18k/221.18k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2 (--/700.42k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1 (--/92.16k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0a_1x1/weights (1x1x576x160, 92.16k/92.16k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3 (--/276.48k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0b_3x3/weights (3x3x160x192, 276.48k/276.48k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3 (--/331.78k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_2/Conv2d_0c_3x3/weights (3x3x192x192, 331.78k/331.78k params)\n","        FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3 (--/55.30k params)\n","          FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1 (--/55.30k params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            FirstStageFeatureExtractor/InceptionV2/Mixed_4e/Branch_3/Conv2d_0b_1x1/weights (1x1x576x96, 55.30k/55.30k params)\n","  SecondStageBoxPredictor (--/6.15k params)\n","    SecondStageBoxPredictor/BoxEncodingPredictor (--/4.10k params)\n","      SecondStageBoxPredictor/BoxEncodingPredictor/biases (4, 4/4 params)\n","      SecondStageBoxPredictor/BoxEncodingPredictor/weights (1024x4, 4.10k/4.10k params)\n","    SecondStageBoxPredictor/ClassPredictor (--/2.05k params)\n","      SecondStageBoxPredictor/ClassPredictor/biases (2, 2/2 params)\n","      SecondStageBoxPredictor/ClassPredictor/weights (1024x2, 2.05k/2.05k params)\n","  SecondStageFeatureExtractor (--/5.89m params)\n","    SecondStageFeatureExtractor/InceptionV2 (--/5.89m params)\n","      SecondStageFeatureExtractor/InceptionV2/Mixed_5a (--/1.44m params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0 (--/294.91k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1 (--/73.73k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_0a_1x1/weights (1x1x576x128, 73.73k/73.73k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3 (--/221.18k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_0/Conv2d_1a_3x3/weights (3x3x128x192, 221.18k/221.18k params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1 (--/1.14m params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1 (--/110.59k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0a_1x1/weights (1x1x576x192, 110.59k/110.59k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3 (--/442.37k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_0b_3x3/weights (3x3x192x256, 442.37k/442.37k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3 (--/589.82k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5a/Branch_1/Conv2d_1a_3x3/weights (3x3x256x256, 589.82k/589.82k params)\n","      SecondStageFeatureExtractor/InceptionV2/Mixed_5b (--/2.18m params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0 (--/360.45k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1 (--/360.45k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights (1x1x1024x352, 360.45k/360.45k params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1 (--/749.57k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1 (--/196.61k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights (1x1x1024x192, 196.61k/196.61k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3 (--/552.96k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_1/Conv2d_0b_3x3/weights (3x3x192x320, 552.96k/552.96k params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2 (--/937.98k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1 (--/163.84k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights (1x1x1024x160, 163.84k/163.84k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3 (--/322.56k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights (3x3x160x224, 322.56k/322.56k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3 (--/451.58k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_2/Conv2d_0c_3x3/weights (3x3x224x224, 451.58k/451.58k params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3 (--/131.07k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1 (--/131.07k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights (1x1x1024x128, 131.07k/131.07k params)\n","      SecondStageFeatureExtractor/InceptionV2/Mixed_5c (--/2.28m params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0 (--/360.45k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1 (--/360.45k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights (1x1x1024x352, 360.45k/360.45k params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1 (--/749.57k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1 (--/196.61k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0a_1x1/weights (1x1x1024x192, 196.61k/196.61k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3 (--/552.96k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_1/Conv2d_0b_3x3/weights (3x3x192x320, 552.96k/552.96k params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2 (--/1.04m params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1 (--/196.61k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights (1x1x1024x192, 196.61k/196.61k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3 (--/387.07k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights (3x3x192x224, 387.07k/387.07k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3 (--/451.58k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights (3x3x224x224, 451.58k/451.58k params)\n","        SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3 (--/131.07k params)\n","          SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1 (--/131.07k params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm (--/0 params)\n","            SecondStageFeatureExtractor/InceptionV2/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights (1x1x1024x128, 131.07k/131.07k params)\n","\n","======================End of Report==========================\n","204 ops no flops stats due to incomplete shapes.\n","Parsing Inputs...\n","Incomplete shape.\n","\n","=========================Options=============================\n","-max_depth                  10000\n","-min_bytes                  0\n","-min_peak_bytes             0\n","-min_residual_bytes         0\n","-min_output_bytes           0\n","-min_micros                 0\n","-min_accelerator_micros     0\n","-min_cpu_micros             0\n","-min_params                 0\n","-min_float_ops              1\n","-min_occurrence             0\n","-step                       -1\n","-order_by                   float_ops\n","-account_type_regexes       .*\n","-start_name_regexes         .*\n","-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n","-show_name_regexes          .*\n","-hide_name_regexes          \n","-account_displayed_op_only  true\n","-select                     float_ops\n","-output                     stdout:\n","\n","==================Model Analysis Report======================\n","Incomplete shape.\n","\n","Doc:\n","scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n","flops: Number of float operations. Note: Please read the implementation for the math behind it.\n","\n","Profile:\n","node name | # float_ops\n","_TFProfRoot (--/6.16k flops)\n","  map/while/ToNormalizedCoordinates/Scale/mul_1 (300/300 flops)\n","  SecondStagePostprocessor/map/while/ClipToWindow/Maximum_3 (300/300 flops)\n","  SecondStagePostprocessor/map/while/ClipToWindow/Minimum (300/300 flops)\n","  SecondStagePostprocessor/map/while/ClipToWindow/Minimum_1 (300/300 flops)\n","  SecondStagePostprocessor/map/while/ClipToWindow/Minimum_2 (300/300 flops)\n","  SecondStagePostprocessor/map/while/ClipToWindow/Minimum_3 (300/300 flops)\n","  SecondStagePostprocessor/map/while/ClipToWindow/Maximum_2 (300/300 flops)\n","  SecondStagePostprocessor/map/while/ClipToWindow/Maximum_1 (300/300 flops)\n","  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/Scale/mul (300/300 flops)\n","  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/Scale/mul_1 (300/300 flops)\n","  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/Scale/mul_2 (300/300 flops)\n","  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/Scale/mul_3 (300/300 flops)\n","  SecondStagePostprocessor/map/while/ClipToWindow/Maximum (300/300 flops)\n","  map/while/ToNormalizedCoordinates/Scale/mul (300/300 flops)\n","  map/while/ToNormalizedCoordinates/Scale/mul_2 (300/300 flops)\n","  map_2/while/mul_3 (300/300 flops)\n","  map_2/while/mul_2 (300/300 flops)\n","  map_2/while/mul_1 (300/300 flops)\n","  map_2/while/mul (300/300 flops)\n","  map/while/ToNormalizedCoordinates/Scale/mul_3 (300/300 flops)\n","  GridAnchorGenerator/mul (12/12 flops)\n","  GridAnchorGenerator/mul_1 (12/12 flops)\n","  GridAnchorGenerator/mul_2 (12/12 flops)\n","  GridAnchorGenerator/truediv (12/12 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_18 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_19 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_9 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n","  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/truediv_1 (1/1 flops)\n","  mul (1/1 flops)\n","  map_2/while/Less_1 (1/1 flops)\n","  map_2/while/Less (1/1 flops)\n","  map_1/while/ToNormalizedCoordinates/truediv_1 (1/1 flops)\n","  map_1/while/ToNormalizedCoordinates/truediv (1/1 flops)\n","  map_1/while/Less_1 (1/1 flops)\n","  map_1/while/Less (1/1 flops)\n","  map/while/ToNormalizedCoordinates/truediv_1 (1/1 flops)\n","  map/while/ToNormalizedCoordinates/truediv (1/1 flops)\n","  map/while/Less_1 (1/1 flops)\n","  map/while/Less (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n","  SecondStagePostprocessor/map/while/ToNormalizedCoordinates/truediv (1/1 flops)\n","  SecondStagePostprocessor/map/while/Less_1 (1/1 flops)\n","  SecondStagePostprocessor/map/while/Less (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub_1 (1/1 flops)\n","  Preprocessor/map/while/ResizeToRange/cond/resize_1/Minimum (1/1 flops)\n","  Preprocessor/map/while/ResizeToRange/cond/resize_1/mul (1/1 flops)\n","  Preprocessor/map/while/ResizeToRange/cond/resize_1/mul_1 (1/1 flops)\n","  Preprocessor/map/while/ResizeToRange/cond/resize_1/truediv (1/1 flops)\n","  Preprocessor/map/while/ResizeToRange/cond/resize_1/truediv_1 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub (1/1 flops)\n","  Preprocessor/map/while/ResizeToRange/cond/resize/truediv_1 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv_1 (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n","  SecondStagePostprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n","  Preprocessor/map/while/ResizeToRange/cond/resize/truediv (1/1 flops)\n","  Preprocessor/map/while/ResizeToRange/cond/resize/mul_1 (1/1 flops)\n","  Preprocessor/map/while/ResizeToRange/cond/resize/mul (1/1 flops)\n","  Preprocessor/map/while/ResizeToRange/cond/resize/Minimum (1/1 flops)\n","  Preprocessor/map/while/ResizeToRange/Less (1/1 flops)\n","  Preprocessor/map/while/Less_1 (1/1 flops)\n","  Preprocessor/map/while/Less (1/1 flops)\n","  GridAnchorGenerator/zeros/Less (1/1 flops)\n","  GridAnchorGenerator/mul_8 (1/1 flops)\n","  GridAnchorGenerator/mul_7 (1/1 flops)\n","  GridAnchorGenerator/assert_equal_1/Equal (1/1 flops)\n","  FirstStageFeatureExtractor/GreaterEqual_1 (1/1 flops)\n","  FirstStageFeatureExtractor/GreaterEqual (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n","  BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n","\n","======================End of Report==========================\n","2021-05-20 12:17:40.727248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n","2021-05-20 12:17:40.737957: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n","2021-05-20 12:17:40.738001: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (b59efc49287c): /proc/driver/nvidia/version does not exist\n","2021-05-20 12:17:40.747074: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","2021-05-20 12:17:40.751844: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n","2021-05-20 12:17:40.752040: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55adb872f9c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n","2021-05-20 12:17:40.752073: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-72\n","I0520 12:17:40.754614 140686366287744 saver.py:1284] Restoring parameters from training/model.ckpt-72\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","W0520 12:17:42.092984 140686366287744 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use standard file APIs to check for files with this prefix.\n","INFO:tensorflow:Restoring parameters from training/model.ckpt-72\n","I0520 12:17:42.651859 140686366287744 saver.py:1284] Restoring parameters from training/model.ckpt-72\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n","W0520 12:17:43.288803 140686366287744 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.extract_sub_graph`\n","W0520 12:17:43.289077 140686366287744 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.extract_sub_graph`\n","INFO:tensorflow:Froze 356 variables.\n","I0520 12:17:43.681043 140686366287744 graph_util_impl.py:334] Froze 356 variables.\n","INFO:tensorflow:Converted 356 variables to const ops.\n","I0520 12:17:43.799898 140686366287744 graph_util_impl.py:394] Converted 356 variables to const ops.\n","WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:384: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n","W0520 12:17:44.433047 140686366287744 deprecation.py:323] From /content/models/research/object_detection/exporter.py:384: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n","INFO:tensorflow:No assets to save.\n","I0520 12:17:44.433907 140686366287744 builder_impl.py:640] No assets to save.\n","INFO:tensorflow:No assets to write.\n","I0520 12:17:44.434056 140686366287744 builder_impl.py:460] No assets to write.\n","INFO:tensorflow:SavedModel written to: ./fine_tuned_model/saved_model/saved_model.pb\n","I0520 12:17:44.981684 140686366287744 builder_impl.py:425] SavedModel written to: ./fine_tuned_model/saved_model/saved_model.pb\n","INFO:tensorflow:Writing pipeline config file to ./fine_tuned_model/pipeline.config\n","I0520 12:17:45.036618 140686366287744 config_util.py:254] Writing pipeline config file to ./fine_tuned_model/pipeline.config\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kWbdA6OlWCXc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621513065787,"user_tz":-600,"elapsed":4732565,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}},"outputId":"cdceb17f-6fac-44cc-b641-82dc252bb284"},"source":["!ls {output_directory}"],"execution_count":24,"outputs":[{"output_type":"stream","text":["checkpoint\t\t\tmodel.ckpt.index  saved_model\n","frozen_inference_graph.pb\tmodel.ckpt.meta\n","model.ckpt.data-00000-of-00001\tpipeline.config\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CpkFCIJTWKSG","executionInfo":{"status":"ok","timestamp":1621513065787,"user_tz":-600,"elapsed":4732564,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}}},"source":["import os\n","\n","pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\")\n","assert os.path.isfile(pb_fname), '`{}` not exist'.format(pb_fname)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"DuBIZMUrWQDO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621513066504,"user_tz":-600,"elapsed":4733280,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}},"outputId":"34981138-a7eb-4692-c44a-8585334b7984"},"source":["!ls -alh {pb_fname}"],"execution_count":26,"outputs":[{"output_type":"stream","text":["-rw-r--r-- 1 root root 50M May 20 12:17 /content/models/research/fine_tuned_model/frozen_inference_graph.pb\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1sm2AYFJWT7T","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621513066505,"user_tz":-600,"elapsed":4733280,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}},"outputId":"1e799d5b-c770-4327-8c52-9c411d991d59"},"source":["import os\n","import glob\n","\n","# Path to frozen detection graph. This is the actual model that is used for the object detection.\n","PATH_TO_CKPT = pb_fname\n","\n","# List of the strings that is used to add correct label for each box.\n","PATH_TO_LABELS = label_map_pbtxt_fname\n","\n","# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n","PATH_TO_TEST_IMAGES_DIR =  os.path.join(repo_dir_path, \"test\")\n","\n","assert os.path.isfile(pb_fname)\n","assert os.path.isfile(PATH_TO_LABELS)\n","TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.*\"))\n","assert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)\n","print(TEST_IMAGE_PATHS)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["['/content/BloodRBC/test/BloodImage_00229.jpeg', '/content/BloodRBC/test/BloodImage_00228.jpeg', '/content/BloodRBC/test/BloodImage_00230.jpeg', '/content/BloodRBC/test/BloodImage_00232.jpeg', '/content/BloodRBC/test/BloodImage_00233.jpeg', '/content/BloodRBC/test/BloodImage_00236.jpeg', '/content/BloodRBC/test/BloodImage_00226.jpeg', '/content/BloodRBC/test/BloodImage_00225.jpeg', '/content/BloodRBC/test/BloodImage_00227.jpeg', '/content/BloodRBC/test/BloodImage_00235.jpeg', '/content/BloodRBC/test/BloodImage_00234.jpeg', '/content/BloodRBC/test/BloodImage_00223.jpeg', '/content/BloodRBC/test/BloodImage_00231.jpeg']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bCjjxvw_YIl7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1621513110633,"user_tz":-600,"elapsed":4777407,"user":{"displayName":"Tri Hoang Nguyen","photoUrl":"","userId":"08708948486318645835"}},"outputId":"19104b4a-25c1-4850-9e75-8c6c5538edc6"},"source":["%cd /content/models/research/object_detection\n","\n","import numpy as np\n","import os\n","import six.moves.urllib as urllib\n","import sys\n","import tarfile\n","import tensorflow as tf\n","import zipfile\n","\n","from collections import defaultdict\n","from io import StringIO\n","from matplotlib import pyplot as plt\n","from PIL import Image\n","\n","# This is needed since the notebook is stored in the object_detection folder.\n","sys.path.append(\"..\")\n","from object_detection.utils import ops as utils_ops\n","\n","\n","# This is needed to display the images.\n","%matplotlib inline\n","\n","\n","from object_detection.utils import label_map_util\n","\n","from object_detection.utils import visualization_utils as vis_util\n","\n","\n","detection_graph = tf.Graph()\n","with detection_graph.as_default():\n","    od_graph_def = tf.GraphDef()\n","    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n","        serialized_graph = fid.read()\n","        od_graph_def.ParseFromString(serialized_graph)\n","        tf.import_graph_def(od_graph_def, name='')\n","\n","\n","label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n","categories = label_map_util.convert_label_map_to_categories(\n","    label_map, max_num_classes=num_classes, use_display_name=True)\n","category_index = label_map_util.create_category_index(categories)\n","\n","\n","def load_image_into_numpy_array(image):\n","    (im_width, im_height) = image.size\n","    return np.array(image.getdata()).reshape(\n","        (im_height, im_width, 3)).astype(np.uint8)\n","\n","# Size, in inches, of the output images.\n","IMAGE_SIZE = (12, 8)\n","\n","\n","def run_inference_for_single_image(image, graph):\n","    with graph.as_default():\n","        with tf.Session() as sess:\n","            # Get handles to input and output tensors\n","            ops = tf.get_default_graph().get_operations()\n","            all_tensor_names = {\n","                output.name for op in ops for output in op.outputs}\n","            tensor_dict = {}\n","            for key in [\n","                'num_detections', 'detection_boxes', 'detection_scores',\n","                'detection_classes', 'detection_masks'\n","            ]:\n","                tensor_name = key + ':0'\n","                if tensor_name in all_tensor_names:\n","                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n","                        tensor_name)\n","            if 'detection_masks' in tensor_dict:\n","                # The following processing is only for single image\n","                detection_boxes = tf.squeeze(\n","                    tensor_dict['detection_boxes'], [0])\n","                detection_masks = tf.squeeze(\n","                    tensor_dict['detection_masks'], [0])\n","                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n","                real_num_detection = tf.cast(\n","                    tensor_dict['num_detections'][0], tf.int32)\n","                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n","                                           real_num_detection, -1])\n","                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n","                                           real_num_detection, -1, -1])\n","                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n","                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n","                detection_masks_reframed = tf.cast(\n","                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n","                # Follow the convention by adding back the batch dimension\n","                tensor_dict['detection_masks'] = tf.expand_dims(\n","                    detection_masks_reframed, 0)\n","            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n","\n","            # Run inference\n","            output_dict = sess.run(tensor_dict,\n","                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n","\n","            # all outputs are float32 numpy arrays, so convert types as appropriate\n","            output_dict['num_detections'] = int(\n","                output_dict['num_detections'][0])\n","            output_dict['detection_classes'] = output_dict[\n","                'detection_classes'][0].astype(np.uint8)\n","            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n","            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n","            if 'detection_masks' in output_dict:\n","                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n","    return output_dict\n","\n","\n","for image_path in TEST_IMAGE_PATHS:\n","    image = Image.open(image_path)\n","    # the array based representation of the image will be used later in order to prepare the\n","    # result image with boxes and labels on it.\n","    image_np = load_image_into_numpy_array(image)\n","    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n","    image_np_expanded = np.expand_dims(image_np, axis=0)\n","    # Actual detection.\n","    output_dict = run_inference_for_single_image(image_np, detection_graph)\n","    # Visualization of the results of a detection.\n","    vis_util.visualize_boxes_and_labels_on_image_array(\n","        image_np,\n","        output_dict['detection_boxes'],\n","        output_dict['detection_classes'],\n","        output_dict['detection_scores'],\n","        category_index,\n","        instance_masks=output_dict.get('detection_masks'),\n","        use_normalized_coordinates=True,\n","        line_thickness=8)\n","    plt.figure(figsize=IMAGE_SIZE)\n","    plt.imshow(image_np)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["/content/models/research/object_detection\n"],"name":"stdout"}]}]}